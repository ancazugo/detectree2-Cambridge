{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/PatBall1/detectree2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, glob, time, json, random, yaml\n",
    "from datetime import date, datetime\n",
    "from pathlib import Path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectree2.preprocessing.tiling import tile_data_train, to_traintest_folders, tile_data\n",
    "from detectree2.models.predict import predict_on_data\n",
    "from detectree2.models.train import MyTrainer, setup_cfg, register_train_data, remove_registered_data, predictions_on_data, combine_dicts, get_tree_dicts, load_json_arr\n",
    "from detectree2.models.outputs import project_to_geojson, stitch_crowns, clean_crowns, to_eval_geojson, clean_predictions\n",
    "from detectree2.models.evaluation import site_f1_score2\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.evaluation.coco_evaluation import instances_to_coco_json\n",
    "\n",
    "import cv2\n",
    "import wandb\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import rioxarray as rxr\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv('DATA_FOLDER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectree_addons import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the training datasets have already been transformmed into COCO format, it is necessary to setup the parameters for all the folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_path = data_dir + \"Cambridge/\"\n",
    "\n",
    "# Set tiling parameters\n",
    "buffer = 0\n",
    "tile_width = 200\n",
    "tile_height = 200\n",
    "threshold = 0\n",
    "tilename = 'city_center'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input paths\n",
    "small_train_dir = site_path + \"train_small/\"\n",
    "small_crown_path = site_path + \"crowns/tiles_0.25m_160_20_0_train_crowns.shp\"\n",
    "small_rgb_path = site_path + \"rgb/\"\n",
    "small_data_name = 'Cambridge_25cm_2017_small'\n",
    "small_tiles_dir = site_path + \"tiles/\"\n",
    "small_train_dir = site_path + \"train/\"\n",
    "small_test_dir = site_path + \"test/\"\n",
    "\n",
    "small_imgs = read_multiple_rgb(small_rgb_path)\n",
    "\n",
    "# Read in crowns (then filter by an attribute if required)\n",
    "small_crowns = gpd.read_file(small_crown_path)\n",
    "small_crowns = small_crowns.to_crs(small_imgs[0].crs.data)\n",
    "\n",
    "# remove_registered_data(data_name)\n",
    "register_train_data(small_train_dir, small_data_name, val_fold=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up input paths\n",
    "large_train_dir = site_path + \"train_large/\"\n",
    "large_crown_path = site_path + \"crowns/tiles_0.25m_160_20_0_train_crowns.shp\"\n",
    "large_rgb_path = site_path + \"rgb/\"\n",
    "large_data_name = 'Cambridge_25cm_2017_large'\n",
    "large_tiles_dir = site_path + \"tiles/\"\n",
    "large_train_dir = site_path + \"train/\"\n",
    "large_test_dir = site_path + \"test/\"\n",
    "\n",
    "large_imgs = read_multiple_rgb(large_rgb_path)\n",
    "\n",
    "# Read in crowns (then filter by an attribute if required)\n",
    "large_crowns = gpd.read_file(large_crown_path)\n",
    "large_crowns = large_crowns.to_crs(large_imgs[0].crs.data)\n",
    "\n",
    "# remove_registered_data(data_name)\n",
    "register_train_data(large_train_dir, large_data_name, val_fold=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it is necessary to change the parameters for the different configurations, for the size of the training dataset and the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'half_dataset' # Select dataset\n",
    "model_name = 'randresize' # Select model\n",
    "data_name = large_data_name if dataset == 'full_dataset' else small_data_name # Trainning data name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base (pre-trained) model from the detectron2 model_zoo\n",
    "time_now = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n",
    "\n",
    "models = {'coco': \"COCO-InstanceSegmentation/mask_rcnn_R_101_FPN_3x.yaml\",\n",
    "          'paracou': data_dir + \"models/220723_withParacouUAV.pth\",\n",
    "          'randresize': data_dir + \"models/230103_randresize_full.pth\"}\n",
    "\n",
    "base_model = models[model_name]\n",
    "output_dir = data_dir + \"Cambridge/0.25m_160_20_0_models/\"\n",
    "train_out_dir = output_dir + f\"{time_now}_{model_name}/\"\n",
    "\n",
    "trains = (f\"{data_name}_train\",) # Registered train data\n",
    "tests = (f\"{data_name}_val\",) # Registered validation data\n",
    "\n",
    "if model_name == 'coco':\n",
    "    cfg = setup_cfg(base_model, trains, tests, eval_period=100,\n",
    "                    max_iter=3000, out_dir=train_out_dir,\n",
    "                    **params[dataset][model_name])\n",
    "    \n",
    "else:\n",
    "    cfg = setup_cfg(update_model=base_model, trains=trains, tests=tests,\n",
    "                    eval_period=100, max_iter=10000, out_dir=train_out_dir,\n",
    "                    **params[dataset][model_name])\n",
    "\n",
    "cfg_wandb = yaml.safe_load(cfg.dump())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thi is the configuration dictionary for the wandb sweep using Bayesian Search to maximise the segmentation AP50 metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter sweep configuration (more info the the W&B docs)\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "      'name': 'segm/AP50',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'backbone_freeze_at':\n",
    "            {'distribution': 'int_uniform',\n",
    "            'max': 4,\n",
    "            'min': 1},\n",
    "        'base_lr':\n",
    "            {'distribution': 'uniform',\n",
    "            'max': 0.025,\n",
    "            'min': 0.00025},\n",
    "        'batch_size_per_image':\n",
    "            {'distribution': 'int_uniform',\n",
    "            'max': 2048,\n",
    "            'min': 512},\n",
    "        'dl_num_workers':\n",
    "            {'distribution': 'int_uniform',\n",
    "            'max': 8,\n",
    "            'min': 1},\n",
    "        'gamma':\n",
    "            {'distribution': 'uniform',\n",
    "            'max': 0.3,\n",
    "            'min': 0.05},\n",
    "        'warmup_iters':\n",
    "            {'distribution': 'int_uniform',\n",
    "            'max': 200,\n",
    "            'min': 50},\n",
    "        'weight_decay':\n",
    "            {'distribution': 'uniform',\n",
    "            'max': 0.1,\n",
    "            'min': 0.001}\n",
    "    }\n",
    "}\n",
    "\n",
    "#initialize the sweep\n",
    "#Running this line will ask you to log into your W&B account\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"detectree2-Cambridge\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will start a wandb run, assuming you have logged in with `wandb login` and have a project set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"detectree2-Cambridge\",\n",
    "    sync_tensorboard=True,\n",
    "    # track hyperparameters and run metadata\n",
    "    config = cfg_wandb\n",
    ")\n",
    "print(run.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a wrapper around the trainer and wandb Sweeps. This will perform the training for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    run = wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"detectree2-Cambridge\",\n",
    "    sync_tensorboard=True,\n",
    "    # track hyperparameters and run metadata\n",
    "    config = cfg_wandb)\n",
    "\n",
    "    trainer = MyTrainer(cfg, patience=5)\n",
    "    trainer.resume_or_load(resume=False)\n",
    "    trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run the wandb sweep. It is the longest cell to run, depending on the configuration of the sweep and the training dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, run, count=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monitor the performance of the sweep directly in the W&B website of the report."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
